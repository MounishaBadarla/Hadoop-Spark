login as: badarlma
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
badarlma@hadoop-gate-0:~$ pyspark
Python 2.7.12 (default, Oct  8 2019, 14:14:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLeve                                                                                        l(newLevel).
20/04/14 02:04:18 WARN Utils: Service 'SparkUI' could not bind on port 4040. Att                                                                                        empting port 4041.
20/04/14 02:04:18 WARN Utils: Service 'SparkUI' could not bind on port 4041. Att                                                                                        empting port 4042.
20/04/14 02:04:18 WARN Utils: Service 'SparkUI' could not bind on port 4042. Att                                                                                        empting port 4043.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.0.0-78
      /_/

Using Python version 2.7.12 (default, Oct  8 2019 14:14:10)
SparkSession available as 'spark'.
>>> filepath2010 = "/data/weather/2010"
>>> filepath2011 = "/data/weather/2011"
>>> filepath2012 = "/data/weather/2012"
>>> filepath2013 = "/data/weather/2013"
>>> filepath2014 = "/data/weather/2014"
>>> TextData2010 = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(filepath2010)
>>> TextData2011 = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(filepath2011)
20/04/14 02:07:33 WARN SharedInMemoryCache: Evicting cached table partition metadata from memory due to size constraints (spark.sql.hive.filesourcePartitionFileCacheSize = 262144000 bytes). This may impact query planning performance.
>>> TextData2012 = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(filepath2012)
>>> TextData2013 = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(filepath2013)
>>> TextData2014 = spark.read.format("csv").option("header", "true").option("delimiter", "\t").load(filepath2014)
>>> def Columndrop(k):
...     columns = k.split(" ")
...     result = columns[0] + " " + columns[2] + " " + columns[3] + " " + columns[5] + " " + columns[7] + " " + columns[
...         9] + " " + columns[11] + " " + columns[13] + " " + columns[15] + " " + columns[16] + " " + columns[17] + " " + \
...              columns[18] + " " + columns[19] + " " + columns[20] + " " + columns[21]
...     return result
...
>>> def DataCleaningProc(k,fname):
...     from pyspark.sql.functions import col, split
...     rdd01 = k.rdd
...     rdd02 = rdd01.map(lambda k: str(k).split('=')[1])
...     rdd03 = rdd02.map(lambda k: ' '.join(k.split()))
...     rdd04 = rdd03.map(lambda k: k[1:-2])
...     rdd05 = rdd04.map(lambda k: k.replace("'", ""))
...     rdd06 = rdd05.map(Columndrop)
...     filespath = "/user/badarlma/"+fname
...     rdd06.saveAsTextFile(filespath)
...     newfileData = spark.read.csv(filespath , header=False, sep=' ')
...     cleanedTableData1 = newfileData.withColumnRenamed('_c0', 'STN').withColumnRenamed('_c1', 'YEARMODA') \
...         .withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DEWP') \
...         .withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP') \
...         .withColumnRenamed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP') \
...         .withColumnRenamed('_c8', 'MXSPD').withColumnRenamed('_c9', 'GUST') \
...         .withColumnRenamed('_c10', 'MAX').withColumnRenamed('_c11', 'MIN') \
...         .withColumnRenamed('_c12', 'PRCP').withColumnRenamed('_c13', 'SNDP') \
...         .withColumnRenamed('_c14', 'FRSHTT')
...     cleanedTableData2 = cleanedTableData1.withColumn("MIN", split(col("MIN"), "\\*").getItem(0))
...     cleanedTableData3 = cleanedTableData2.withColumn("MAX", split(col("MAX"), "\\*").getItem(0))
...     cleanedTableData3 = cleanedTableData3.withColumn("MAX_NEW", cleanedTableData3["MAX"].cast("double"))
...     cleanedTableData3 = cleanedTableData3.withColumn("MIN_NEW", cleanedTableData3["MIN"].cast("double"))
...     cleanedTableData3 = cleanedTableData3.drop('MAX', 'MIN')
...     cleanedTableData3 = cleanedTableData3.withColumnRenamed("MAX_NEW", "MAX")
...     cleanedTableData3 = cleanedTableData3.withColumnRenamed("MIN_NEW", "MIN")
...     cleanedTableData4 = cleanedTableData3.filter(cleanedTableData3.TEMP <> 9999.9)
...     cleanedTableData4 = cleanedTableData3.filter(cleanedTableData3.MAX <> 9999.9)
...     cleanedTableData4 = cleanedTableData3.filter(cleanedTableData3.MIN <> 9999.9)
...     return cleanedTableData4
...
>>> TextData2010_2014 = TextData2010.union(TextData2011).union(TextData2012).union(TextData2013).union(TextData2014).distinct()
>>> cleanedData1 = DataCleaningProc(TextData2010_2014,"filepath2010_2014")
>>> exit()
badarlma@hadoop-gate-0:~$ hdfs dfs -ls "/user/badarlma/"
Found 6 items
drwxr-xr-x   - badarlma hdfs          0 2020-04-13 23:38 /user/badarlma/\useadarlma\cleanedData1
drwxr-xr-x   - badarlma hdfs          0 2020-04-14 00:04 /user/badarlma/\useadarlma\cleanedData2
drwx------   - badarlma hdfs          0 2020-04-14 02:01 /user/badarlma/.Trash
drwxr-xr-x   - badarlma hdfs          0 2020-04-14 02:23 /user/badarlma/.sparkStaging
drwxr-xr-x   - badarlma hdfs          0 2020-04-14 02:23 /user/badarlma/filepath2010_2014
drwxr-xr-x   - badarlma hdfs          0 2020-04-11 19:04 /user/badarlma/temp
badarlma@hadoop-gate-0:~$
login as: badarlma
Keyboard-interactive authentication prompts from server:
| Password:
End of keyboard-interactive prompts from server
badarlma@hadoop-gate-0:~$ pyspark
Python 2.7.12 (default, Oct  8 2019, 14:14:10)
[GCC 5.4.0 20160609] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLeve                                                                                        l(newLevel).
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4040. Att                                                                                        empting port 4041.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4041. Att                                                                                        empting port 4042.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4042. Att                                                                                        empting port 4043.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4043. Att                                                                                        empting port 4044.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4044. Att                                                                                        empting port 4045.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4045. Att                                                                                        empting port 4046.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4046. Att                                                                                        empting port 4047.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4047. Att                                                                                        empting port 4048.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4048. Att                                                                                        empting port 4049.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4049. Att                                                                                        empting port 4050.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4050. Att                                                                                        empting port 4051.
20/04/14 22:12:56 WARN Utils: Service 'SparkUI' could not bind on port 4051. Att                                                                                        empting port 4052.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.3.2.3.1.0.0-78
      /_/

Using Python version 2.7.12 (default, Oct  8 2019 14:14:10)
SparkSession available as 'spark'.
>>> filepath1014 = "/user/badarlma/cleanedData1"
>>> filepath1519 = "/user/badarlma/cleanedData2"
>>> Data1 = spark.read.csv(filepath1014)
>>> Data2 = spark.read.csv(filepath1519)
>>> finalData = Data1.union(Data2).distinct()
cleanDataLast = finalData.withColumnRenamed('_c0', 'STN').withColumnRenamed('_c1                                                                                        ', 'YEARMODA').withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DEWP')                                                                                        .withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP').withColumnRenam                                                                                        ed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP').withColumnRenamed('_c8', 'MX                                                                                        SPD').withColumnRenamed('_c9', 'GUST').withColumnRenamed('_c10', 'PRCP').withCol                                                                                        umnRenamed('_c11', 'SNDP').withColumnRenamed('_c12', 'FRSHTT').withColumnRenamed                                                                                        ('_c13', 'MAX').withColumnRenamed('_c14', 'MIN')
cleanDataLast.createOrReplaceTempView("finalTable")
cleanDataLast = cleanDataLast.withColumn("MAX_NEW", cleanDataLast["MAX"].cast("d                                                                                        ouble"))
cleanDataLast = cleanDataLast.withColumn("MIN_NEW", cleanDataLast["MIN"].cast("d                                                                                        ouble"))
cleanDataLast = cleanDataLast.drop('MAX', 'MIN')
cleanDataLast = cleanDataLast.withColumnRenamed("MAX_NEW", "MAX")
cleanDataLast = cleanDataLast.withColumnRenamed("MIN_NEW", "MIN")
cleanDataLast.createOrReplaceTempView("finalTable")>>> cleanDataLast = finalData                                                                                        '_c1', 'YEARMODA').withColumnRenamed('_c2', 'TEMP').withColumnRenamed('_c3', 'DE                                                                                        WP').withColumnRenamed('_c4', 'SLP').withColumnRenamed('_c5', 'STP').withColumnR                                                                                        enamed('_c6', 'VISIB').withColumnRenamed('_c7', 'WDSP').withColumnRenamed('_c8',                                                                                         'MXSPD').withColumnRenamed('_c9', 'GUST').withColumnRenamed('_c10', 'PRCP').wit                                                                                        hColumnRenamed('_c11', 'SNDP').withColumnRenamed('_c12', 'FRSHTT').withColumnRen                                                                                        amed('_c13', 'MAX').withColumnRenamed('_c14', 'MIN')
>>> cleanDataLast.createOrReplaceTempView("finalTable")
>>> cleanDataLast = cleanDataLast.withColumn("MAX_NEW", cleanDataLast["MAX"].cas                                                                                        t("double"))
>>> cleanDataLast = cleanDataLast.withColumn("MIN_NEW", cleanDataLast["MIN"].cas                                                                                        t("double"))
>>> cleanDataLast = cleanDataLast.drop('MAX', 'MIN')
>>> cleanDataLast = cleanDataLast.withColumnRenamed("MAX_NEW", "MAX")
>>> cleanDataLast = cleanDataLast.withColumnRenamed("MIN_NEW", "MIN")
>>> cleanDataLast.createOrReplaceTempView("finalTable")
>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2012') and LEFT(YEARMODA,4) ='2012' ").show()
Hive Session ID = 22a1498d-1d93-4fde-ace9-49f22482ed04
20/04/14 22:15:53 ERROR JniBasedUnixGroupsMapping: error looking up the name of group 1013495786: No such file or directory
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2012|   09|  16|-119.6|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2013') and LEFT(YEARMODA,4) ='2013' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|895770|2013|   07|  31|-115.1|
|895770|2013|   07|  30|-115.1|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2014') and LEFT(YEARMODA,4) ='2014' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2014|   08|  21|-113.4|
+------+----+-----+----+------+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|895770|2015|   09|  17|-114.2|
|896060|2015|   08|  22|-114.2|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2016') and LEFT(YEARMODA,4) ='2016' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2016|   07|  12|-115.1|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2017') and LEFT(YEARMODA,4) ='2017' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896250|2017|   06|  20|-116.0|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2018') and LEFT(YEARMODA,4) ='2018' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2018|   08|  28|-116.3|
+------+----+-----+----+------+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where Min in (Select MIN(MIN) from finalTable where Min <>999.9 and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4) ='2019' ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2019|   04|  05|-102.1|
+------+----+-----+----+------+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 9999.9) ").show()
[Stage 147:===================================================> (195 + 2) / 200]Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/usr/hdp/current/spark2-client/python/pyspark/sql/dataframe.py", line 350, in show
[Stage 148:>                                                        (0 + 0) / 1]    print(self._jdf.showString(n, 20, vertical))
  File "/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/usr/hdp/current/spark2-client/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o113.showString.
: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange hashpartitioning(STN#96, YEAR#1170, MONTH#1171, DATE#1172, MAX#386, 200)
+- *(14) HashAggregate(keys=[STN#96, YEAR#1170, MONTH#1171, DATE#1172, MAX#386], functions=[], output=[STN#96, YEAR#1170, MONTH#1171, DATE#1172, MAX#386])
   +- *(14) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[STN#96, YEAR#1170, MONTH#1171, DATE#1172, MAX#386])
      +- Exchange hashpartitioning(_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16, 200)
         +- *(13) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16])
            +- Union
               :- *(6) BroadcastHashJoin [cast(_c13#23 as double)], [max(MAX)#1175], LeftSemi, BuildRight
               :  :- *(6) FileScan csv [_c0#10,_c1#11,_c2#12,_c3#13,_c4#14,_c5#15,_c6#16,_c7#17,_c8#18,_c9#19,_c10#20,_c11#21,_c12#22,_c13#23,_c14#24] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
               :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, double, true]))
               :     +- *(5) HashAggregate(keys=[], functions=[max(MAX#386)], output=[max(MAX)#1175])
               :        +- Exchange SinglePartition
               :           +- *(4) HashAggregate(keys=[], functions=[partial_max(MAX#386)], output=[max#1198])
               :              +- *(4) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[MAX#386])
               :                 +- Exchange hashpartitioning(_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16, 200)
               :                    +- *(3) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16])
               :                       +- Union
               :                          :- *(1) Project [_c0#10, _c1#11, _c2#12, _c3#13, _c4#14, _c5#15, _c6#16, _c7#17, _c8#18, _c9#19, _c10#20, _c11#21, _c12#22, _c13#23, _c14#24]
               :                          :  +- *(1) Filter (isnotnull(_c13#23) && NOT (cast(_c13#23 as double) = 9999.9))
               :                          :     +- *(1) FileScan csv [_c0#10,_c1#11,_c2#12,_c3#13,_c4#14,_c5#15,_c6#16,_c7#17,_c8#18,_c9#19,_c10#20,_c11#21,_c12#22,_c13#23,_c14#24] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData1], PartitionFilters: [], PushedFilters: [IsNotNull(_c13)], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
               :                          +- *(2) Project [_c0#50, _c1#51, _c2#52, _c3#53, _c4#54, _c5#55, _c6#56, _c7#57, _c8#58, _c9#59, _c10#60, _c11#61, _c12#62, _c13#63, _c14#64]
               :                             +- *(2) Filter (isnotnull(_c13#63) && NOT (cast(_c13#63 as double) = 9999.9))
               :                                +- *(2) FileScan csv [_c0#50,_c1#51,_c2#52,_c3#53,_c4#54,_c5#55,_c6#56,_c7#57,_c8#58,_c9#59,_c10#60,_c11#61,_c12#62,_c13#63,_c14#64] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData2], PartitionFilters: [], PushedFilters: [IsNotNull(_c13)], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
               +- *(12) BroadcastHashJoin [cast(_c13#63 as double)], [max(MAX)#1175], LeftSemi, BuildRight
                  :- *(12) FileScan csv [_c0#50,_c1#51,_c2#52,_c3#53,_c4#54,_c5#55,_c6#56,_c7#57,_c8#58,_c9#59,_c10#60,_c11#61,_c12#62,_c13#63,_c14#64] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
                  +- ReusedExchange [max(MAX)#1175], BroadcastExchange HashedRelationBroadcastMode(List(input[0, double, true]))

        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)
        at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)
        at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
        at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)
        at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
        at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)
        at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
        at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)
        at org.apache.spark.sql.Dataset.head(Dataset.scala:2489)
        at org.apache.spark.sql.Dataset.take(Dataset.scala:2703)
        at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
        at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
        at py4j.Gateway.invoke(Gateway.java:282)
        at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
        at py4j.commands.CallCommand.execute(CallCommand.java:79)
        at py4j.GatewayConnection.run(GatewayConnection.java:238)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:
Exchange hashpartitioning(_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16, 200)
+- *(13) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16])
   +- Union
      :- *(6) BroadcastHashJoin [cast(_c13#23 as double)], [max(MAX)#1175], LeftSemi, BuildRight
      :  :- *(6) FileScan csv [_c0#10,_c1#11,_c2#12,_c3#13,_c4#14,_c5#15,_c6#16,_c7#17,_c8#18,_c9#19,_c10#20,_c11#21,_c12#22,_c13#23,_c14#24] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData1], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
      :  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, double, true]))
      :     +- *(5) HashAggregate(keys=[], functions=[max(MAX#386)], output=[max(MAX)#1175])
      :        +- Exchange SinglePartition
      :           +- *(4) HashAggregate(keys=[], functions=[partial_max(MAX#386)], output=[max#1198])
      :              +- *(4) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[MAX#386])
      :                 +- Exchange hashpartitioning(_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16, 200)
      :                    +- *(3) HashAggregate(keys=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16], functions=[], output=[_c11#21, _c13#23, _c3#13, _c5#15, _c10#20, _c0#10, _c4#14, _c9#19, _c14#24, _c8#18, _c1#11, _c7#17, _c2#12, _c12#22, _c6#16])
      :                       +- Union
      :                          :- *(1) Project [_c0#10, _c1#11, _c2#12, _c3#13, _c4#14, _c5#15, _c6#16, _c7#17, _c8#18, _c9#19, _c10#20, _c11#21, _c12#22, _c13#23, _c14#24]
      :                          :  +- *(1) Filter (isnotnull(_c13#23) && NOT (cast(_c13#23 as double) = 9999.9))
      :                          :     +- *(1) FileScan csv [_c0#10,_c1#11,_c2#12,_c3#13,_c4#14,_c5#15,_c6#16,_c7#17,_c8#18,_c9#19,_c10#20,_c11#21,_c12#22,_c13#23,_c14#24] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData1], PartitionFilters: [], PushedFilters: [IsNotNull(_c13)], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
      :                          +- *(2) Project [_c0#50, _c1#51, _c2#52, _c3#53, _c4#54, _c5#55, _c6#56, _c7#57, _c8#58, _c9#59, _c10#60, _c11#61, _c12#62, _c13#63, _c14#64]
      :                             +- *(2) Filter (isnotnull(_c13#63) && NOT (cast(_c13#63 as double) = 9999.9))
      :                                +- *(2) FileScan csv [_c0#50,_c1#51,_c2#52,_c3#53,_c4#54,_c5#55,_c6#56,_c7#57,_c8#58,_c9#59,_c10#60,_c11#61,_c12#62,_c13#63,_c14#64] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData2], PartitionFilters: [], PushedFilters: [IsNotNull(_c13)], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
      +- *(12) BroadcastHashJoin [cast(_c13#63 as double)], [max(MAX)#1175], LeftSemi, BuildRight
         :- *(12) FileScan csv [_c0#50,_c1#51,_c2#52,_c3#53,_c4#54,_c5#55,_c6#56,_c7#57,_c8#58,_c9#59,_c10#60,_c11#61,_c12#62,_c13#63,_c14#64] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://hdfs-0-0.eecscluster:8020/user/badarlma/cleanedData2], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<_c0:string,_c1:string,_c2:string,_c3:string,_c4:string,_c5:string,_c6:string,_c7:string,_c...
         +- ReusedExchange [max(MAX)#1175], BroadcastExchange HashedRelationBroadcastMode(List(input[0, double, true]))

        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        ... 39 more
Caused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)
        at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)
        at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenSemi(BroadcastHashJoinExec.scala:356)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:104)
        at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)
        at org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:158)
        at org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:166)
        at org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:80)
        at org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:158)
        at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)
        at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)
        at org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:158)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)
        at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)
        at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:557)
        at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:557)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
        at scala.collection.immutable.List.foreach(List.scala:381)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
        at scala.collection.immutable.List.map(List.scala:285)
        at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:557)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:371)
        at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:150)
        at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:605)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.prepareShuffleDependency(ShuffleExchangeExec.scala:92)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:128)
        at org.apache.spark.sql.execution.exchange.ShuffleExchangeExec$$anonfun$doExecute$1.apply(ShuffleExchangeExec.scala:119)
        at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)
        ... 60 more

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 999.9) ").show()
+------+----+-----+----+-----+
|   STN|YEAR|MONTH|DATE|  MAX|
+------+----+-----+----+-----+
|720667|2010|   09|  23|132.8|
|406890|2013|   07|  12|132.8|
|722577|2012|   07|  12|132.8|
+------+----+-----+----+-----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where MIN IN(SELECT MIN(MIN) FROM finalTable where MIN <> 999.9) ").show()
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> >>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 999.9) ").show()
  File "<stdin>", line 1
    >>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 999.9) ").show()
     ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> |   STN|YEAR|MONTH|DATE|  MAX|
  File "<stdin>", line 1
    |   STN|YEAR|MONTH|DATE|  MAX|
    ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> |720667|2010|   09|  23|132.8|
  File "<stdin>", line 1
    |720667|2010|   09|  23|132.8|
    ^
SyntaxError: invalid syntax
>>> |406890|2013|   07|  12|132.8|
  File "<stdin>", line 1
    |406890|2013|   07|  12|132.8|
    ^
SyntaxError: invalid syntax
>>> |722577|2012|   07|  12|132.8|
  File "<stdin>", line 1
    |722577|2012|   07|  12|132.8|
    ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> >>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 999.9) ").show()
  File "<stdin>", line 1
    >>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MAX from finalTable where MAX IN(SELECT MAX(MAX) FROM finalTable where MAX <> 999.9) ").show()
     ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> |   STN|YEAR|MONTH|DATE|  MAX|
  File "<stdin>", line 1
    |   STN|YEAR|MONTH|DATE|  MAX|
    ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> |720667|2010|   09|  23|132.8|
  File "<stdin>", line 1
    |720667|2010|   09|  23|132.8|
    ^
SyntaxError: invalid syntax
>>> |406890|2013|   07|  12|132.8|
  File "<stdin>", line 1
    |406890|2013|   07|  12|132.8|
    ^
SyntaxError: invalid syntax
>>> |722577|2012|   07|  12|132.8|
  File "<stdin>", line 1
    |722577|2012|   07|  12|132.8|
    ^
SyntaxError: invalid syntax
>>> +------+----+-----+----+-----+
  File "<stdin>", line 1
    +------+----+-----+----+-----+
                                 ^
SyntaxError: invalid syntax
>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, MIN from finalTable where MIN IN(SELECT MIN(MIN) FROM finalTable where MIN <> 999.9) ").show()
+------+----+-----+----+------+
|   STN|YEAR|MONTH|DATE|   MIN|
+------+----+-----+----+------+
|896060|2012|   09|  16|-119.6|
+------+----+-----+----+------+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> from pyspark.sql.functions import col, split
>>> cleanDataLast=cleanDataLast.withColumn("PRCP", split(col("PRCP"), "[A-Z]").getItem(0)).withColumn("P1", split(col("PRCP"), "[A-Z]").getItem(1))
>>> cleanDataLast.createOrReplaceTempView("finalTable")
>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from finalTable where PRCP in (SELECT MAX(PRCP) from finalTable where PRCP <>'99.99' and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|PRCP|
+------+----+-----+----+----+
|983340|  16|   12|2015|9.92|
|418621|  21|   08|2015|9.92|
|645510|  23|   11|2015|9.92|
|419890|  24|   07|2015|9.92|
+------+----+-----+----+----+
>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from finalTable where PRCP in (SELECT MAX(PRCP) from finalTable where PRCP <>'99.99' and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+-----+
|   STN|DATE|MONTH|YEAR| PRCP|
+------+----+-----+----+-----+
|915410|  11|   03|2015|19.49|
+------+----+-----+----+-----+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH, LEFT(YEARMODA,4) as YEAR, PRCP  from finalTable where PRCP in (SELECT MIN(PRCP) from finalTable where PRCP <>'99.99' and LEFT(YEARMODA,4)='2015') and LEFT(YEARMODA,4) ='2015' ").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|PRCP|
+------+----+-----+----+----+
|724110|  03|   03|2015|0.00|
|724945|  14|   04|2015|0.00|
|720631|  05|   07|2015|0.00|
|722124|  16|   02|2015|0.00|
|724768|  17|   07|2015|0.00|
|010920|  01|   07|2015|0.00|
|167193|  10|   07|2015|0.00|
|260720|  15|   07|2015|0.00|
|350670|  31|   12|2015|0.00|
|407450|  09|   02|2015|0.00|
|407570|  18|   11|2015|0.00|
|954800|  14|   07|2015|0.00|
|605170|  08|   10|2015|0.00|
|624350|  19|   09|2015|0.00|
|804470|  21|   12|2015|0.00|
|415360|  29|   12|2015|0.00|
|713750|  22|   05|2015|0.00|
|153410|  07|   08|2015|0.00|
|864600|  19|   06|2015|0.00|
|607390|  26|   02|2015|0.00|
+------+----+-----+----+----+
only showing top 20 rows

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> missingdf=cleanDataLast.where("STP='9999.9'")
>>> missingtotal=(float)(missingdf.count())
>>> total=(float)(cleanDataLast.count())
>>> percent=(missingtotal*100)/(total)
>>> print(percent)
32.4612917675
>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> spark.sql("Select STN,RIGHT(YEARMODA,2)as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST from finalTable where GUST in (Select MAX(GUST) from finalTable where GUST <>'999.9')").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|GUST|
+------+----+-----+----+----+
|726130|  29|   12|2011|99.8|
|726130|  18|   03|2011|99.8|
|479310|  29|   09|2012|99.8|
+------+----+-----+----+----+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST  from finalTable where GUST in (SELECT MAX(GUST) from finalTable where GUST <>'99.99' and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4) ='2019' ").show()
+------+----+-----+----+-----+
|   STN|DATE|MONTH|YEAR| GUST|
+------+----+-----+----+-----+
|172460|  24|   02|2019|999.9|
|386960|  01|   04|2019|999.9|
|506320|  29|   01|2019|999.9|
|843770|  27|   03|2019|999.9|
|875760|  04|   01|2019|999.9|
|949250|  24|   02|2019|999.9|
|800350|  29|   01|2019|999.9|
|725945|  28|   01|2019|999.9|
|085600|  10|   03|2019|999.9|
|119780|  25|   03|2019|999.9|
|172043|  15|   01|2019|999.9|
|217210|  11|   01|2019|999.9|
|307810|  29|   03|2019|999.9|
|471890|  07|   01|2019|999.9|
|542920|  01|   02|2019|999.9|
|943810|  20|   02|2019|999.9|
|082330|  08|   01|2019|999.9|
|653760|  21|   03|2019|999.9|
|012620|  01|   04|2019|999.9|
|999999|  24|   02|2019|999.9|
+------+----+-----+----+-----+
only showing top 20 rows

>>> spark.sql("Select STN,RIGHT(YEARMODA,2) as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST  from finalTable where GUST in (SELECT MAX(GUST) from finalTable where GUST <>'999.99' and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4) ='2019' ").show()
+------+----+-----+----+-----+
|   STN|DATE|MONTH|YEAR| GUST|
+------+----+-----+----+-----+
|172460|  24|   02|2019|999.9|
|386960|  01|   04|2019|999.9|
|506320|  29|   01|2019|999.9|
|843770|  27|   03|2019|999.9|
|875760|  04|   01|2019|999.9|
|949250|  24|   02|2019|999.9|
|800350|  29|   01|2019|999.9|
|725945|  28|   01|2019|999.9|
|085600|  10|   03|2019|999.9|
|119780|  25|   03|2019|999.9|
|172043|  15|   01|2019|999.9|
|217210|  11|   01|2019|999.9|
|307810|  29|   03|2019|999.9|
|471890|  07|   01|2019|999.9|
|542920|  01|   02|2019|999.9|
|943810|  20|   02|2019|999.9|
|082330|  08|   01|2019|999.9|
|653760|  21|   03|2019|999.9|
|012620|  01|   04|2019|999.9|
|999999|  24|   02|2019|999.9|
+------+----+-----+----+-----+
only showing top 20 rows

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt


>>> spark.sql("Select STN,RIGHT(YEARMODA,2)as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST from finalTable where GUST in (Select MAX(GUST) from finalTable where GUST <>'999.9' and LEFT(YEARMODA,4)='2019')").show()
+------+----+-----+----+----+
|   STN|DATE|MONTH|YEAR|GUST|
+------+----+-----+----+----+
|721009|  21|   12|2016|99.1|
|726130|  14|   11|2018|99.1|
|726130|  14|   03|2017|99.1|
|680040|  15|   07|2015|99.1|
|726130|  19|   11|2017|99.1|
|683820|  27|   05|2017|99.1|
|721009|  19|   12|2016|99.1|
|013005|  29|   01|2016|99.1|
|477740|  04|   09|2018|99.1|
|010231|  29|   01|2016|99.1|
|467403|  07|   07|2016|99.1|
|680040|  04|   07|2015|99.1|
|680040|  25|   06|2015|99.1|
|726130|  25|   02|2019|99.1|
+------+----+-----+----+----+

>>> spark.sql("SELECT distinct(STN), LEFT(YEARMODA,4) as YEAR, RIGHT(LEFT(YEARMODA,6),2) as MONTH, RIGHT(YEARMODA,2) as DATE, GUST from finalTable where GUST IN(SELECT MAX(GUST) FROM finalTable where GUST <> 999.9) and LEFT(YEARMODA,4) ='2019' ").show()

+---+----+-----+----+----+
|STN|YEAR|MONTH|DATE|GUST|
+---+----+-----+----+----+
+---+----+-----+----+----+

>>>
>>>
>>> missingdf=cleanDataLast.where("STP='9999.9'")
>>> missingtotal=(float)(missingdf.count())
>>> total=(float)(cleanDataLast.count())
>>> percent=(missingtotal*100)/(total)
>>> print(percent)
27.961149261

>>> spark.sql("Select STN,RIGHT(YEARMODA,2)as DATE, RIGHT(LEFT(YEARMODA,6),2) as MONTH,LEFT(YEARMODA,4) as YEAR,GUST from finalTable where GUST in (Select MAX(GUST) from finalTable where GUST <>'999.9' and LEFT(YEARMODA,4)='2019') and LEFT(YEARMODA,4)='2019'").show()
+------+----+-----+----+-----+
|   STN|DATE|MONTH|YEAR| GUST|
+------+----+-----+----+-----+
|085510|  06|   01|2019|116.6|
|085510|  01|   01|2019|116.6|
+------+----+-----+----+-----+

>>>
Traceback (most recent call last):
  File "/usr/hdp/current/spark2-client/python/pyspark/context.py", line 261, in signal_handler
    raise KeyboardInterrupt()
KeyboardInterrupt
>>>
